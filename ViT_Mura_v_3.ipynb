{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './MURA-v1.1/'\n",
    "train_image_paths_csv = \"train_image_paths.csv\"\n",
    "df_train_images_paths = pd.read_csv(os.path.join(path,train_image_paths_csv),dtype=str,header=None)\n",
    "df_train_images_paths.columns = ['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images_paths['label'] = df_train_images_paths['image_path'].map(lambda x: 'positive' if 'positive' in x else 'negative')\n",
    "df_train_images_paths['category'] = df_train_images_paths['image_path'].apply(lambda x: x.split('/')[2])\n",
    "#df_train_images_paths['patientId'] = df_train_images_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_paths_csv = \"valid_image_paths.csv\"\n",
    "df_valid_data_paths = pd.read_csv(os.path.join(path,valid_image_paths_csv),dtype=str,header=None)\n",
    "df_valid_data_paths.columns = ['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_data_paths['label'] = df_valid_data_paths['image_path'].map(lambda x:'positive' if 'positive' in x else 'negative')\n",
    "df_valid_data_paths['category']  = df_valid_data_paths['image_path'].apply(lambda x: x.split('/')[2])\n",
    "#df_valid_data_paths['dir'] =  df_valid_data_paths['image_path'].apply(lambda x: x.split('/')[1])\n",
    "#df_valid_data_paths['patientId']  = df_valid_data_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_data_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByRatio(dataframe,n_samples = int):\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for category in dataframe[\"category\"].unique():\n",
    "        # Filter the DataFrame by category\n",
    "        sub_df = dataframe[dataframe[\"category\"] == category]\n",
    "\n",
    "        # Filter positive and negative samples\n",
    "        pos_samples = sub_df[sub_df[\"label\"] == 'positive'].sample(n=n_samples, replace=False)\n",
    "        neg_samples = sub_df[sub_df[\"label\"] == 'negative'].sample(n=n_samples, replace=False)\n",
    "\n",
    "        # Concatenate positive and negative samples\n",
    "        balanced_sub_df = pd.concat([pos_samples, neg_samples])\n",
    "\n",
    "        # Add to the final balanced DataFrame\n",
    "        balanced_df = pd.concat([balanced_df, balanced_sub_df])\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = splitByRatio(df_train_images_paths,500)\n",
    "balanced_df = balanced_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_balanced = splitByRatio(df_valid_data_paths,100)\n",
    "valid_balanced = valid_balanced.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_train_images_paths.loc[df_train_images_paths['label'] == 'negative']\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images_paths[\"label\"] = df_train_images_paths[\"label\"].replace({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images_paths.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_balanced[\"label\"] = valid_balanced[\"label\"].replace({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feautre extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import prewitt\n",
    "from skimage.filters import gabor\n",
    "from skimage.filters import sobel\n",
    "from skimage.filters import butterworth\n",
    "from skimage.filters import gaussian\n",
    "from skimage.filters import hessian\n",
    "from skimage.filters import median\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, filtered_df.__len__())\n",
    "img = plt.imread(filtered_df['image_path'][idx])\n",
    "plt.imshow(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_img = exposure.equalize_adapthist(img, clip_limit=0.02)\n",
    "plt.imshow(equalized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd, hog_image = hog(equalized_img, orientations=9, pixels_per_cell=(2, 2),\n",
    "                    cells_per_block=(1, 1), visualize=True)\n",
    "plt.imshow(hog_image)\n",
    "fd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prewitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_prewitt = prewitt(equalized_img)\n",
    "plt.imshow(edge_prewitt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_real, gabor_filter = gabor(equalized_img,frequency=0.1)\n",
    "\n",
    "plt.imshow(gabor_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canny_edge = canny(equalized_img,sigma=0)\n",
    "plt.imshow(Canny_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_edge = sobel(equalized_img)\n",
    "plt.imshow(sobel_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butterworth_img = butterworth(equalized_img)\n",
    "plt.imshow(butterworth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_img = gaussian(equalized_img)\n",
    "plt.imshow(gaussian_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_img = hessian(equalized_img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_img = median(equalized_img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.color import rgb2gray\n",
    "class MuraDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, device='cpu'):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.item()\n",
    "        \n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # Convert PIL Image to numpy array\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        image_gray = rgb2gray(image_np)\n",
    "        \n",
    "        # Apply adaptive histogram equalization\n",
    "        image_gray = exposure.equalize_adapthist(image_gray, clip_limit=0.02)\n",
    "\n",
    "        \n",
    "        # Apply Butterworth filter\n",
    "        image_butterworth = butterworth(image_gray)\n",
    "\n",
    "        \n",
    "        # Apply Gaussian filter\n",
    "        image_gaussian = gaussian(image_gray)\n",
    "        \n",
    "        # Combine the two images into one (stack them along the last axis)\n",
    "        # Create a 3 channel image from the butterworth and gaussian filtered images\n",
    "        image_combined = np.stack([image_butterworth, image_gaussian, image_gray], axis=-1)\n",
    "        \n",
    "        image_combined = (image_combined - np.min(image_combined)) / (np.max(image_combined) - np.min(image_combined))\n",
    "        \n",
    "        # Convert to 8-bit image for PIL\n",
    "        image_combined = img_as_ubyte(image_combined)\n",
    "        \n",
    "        # Convert numpy array back to PIL Image\n",
    "        image = Image.fromarray(image_combined)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        image = image.to(self.device)\n",
    "        \n",
    "        label = torch.tensor(label).to(self.device)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_train_images_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\hamad\\Documents\\GP2\\ViT_Mura_v_3.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m batchsize \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     v2\u001b[39m.\u001b[39mRandomHorizontalFlip(p\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     v2\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     v2\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m Mura_transform \u001b[39m=\u001b[39m MuraDataset(df_train_images_paths[\u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m], df_train_images_paths[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], transform\u001b[39m=\u001b[39mtransform,device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(Mura_transform, batch_size\u001b[39m=\u001b[39mbatchsize, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/hamad/Documents/GP2/ViT_Mura_v_3.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m Mura_transform_valid \u001b[39m=\u001b[39m MuraDataset(valid_balanced[\u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m], valid_balanced[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], transform\u001b[39m=\u001b[39mtransform,device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_images_paths' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "batchsize = 32\n",
    "transform = transforms.Compose([\n",
    "    v2.RandomHorizontalFlip(p=1),\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor(),\n",
    "])\n",
    "Mura_transform = MuraDataset(df_train_images_paths[\"image_path\"], df_train_images_paths[\"label\"], transform=transform,device='cuda')\n",
    "train_loader = DataLoader(Mura_transform, batch_size=batchsize, shuffle=True)\n",
    "Mura_transform_valid = MuraDataset(valid_balanced[\"image_path\"], valid_balanced[\"label\"], transform=transform,device='cuda')\n",
    "valid_loader = DataLoader(Mura_transform_valid, batch_size=batchsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# Select a single image from the batch.\n",
    "img = train_features[6]\n",
    "\n",
    "# Select one channel (e.g., the first channel).\n",
    "img_channel1 = img[0].cpu().numpy()\n",
    "\n",
    "# Display the selected channel.\n",
    "plt.imshow(img_channel1, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Alternatively, you can display the second channel\n",
    "# img_channel2 = img[1].cpu().numpy()\n",
    "# plt.imshow(img_channel2, cmap=\"gray\")\n",
    "# plt.show()\n",
    "\n",
    "label = train_labels[0]\n",
    "print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "vit_model = timm.create_model(model_name='vit_base_patch16_224_in21k',num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vit_model.parameters(), lr=5e-7)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 3\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     vit_model.train()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0  # To keep track of correct predictions\n",
    "#     total = 0  # To keep track of total predictions\n",
    "\n",
    "#     batch_accuracies = []  # To store batch accuracies for the epoch\n",
    "\n",
    "#     for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = vit_model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         _, predicted = outputs.max(1)  # Get the index of the max log-probability\n",
    "#         total += labels.size(0)\n",
    "#         correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "#         # Print loss and accuracy for every 584 batches\n",
    "#         if (batch_idx + 1) % 32 == 0:\n",
    "#             batch_accuracy = 100. * correct / total\n",
    "#             batch_accuracies.append(batch_accuracy)  # Store the batch accuracy\n",
    "#             print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Batch Loss: {loss.item():.4f}, Batch Accuracy: {batch_accuracy:.2f}%\")\n",
    "#     scheduler.step()\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     epoch_accuracy = 100. * correct / total  # Convert to percentage\n",
    "\n",
    "#     # Calculate the average of batch accuracies for the epoch\n",
    "#     avg_batch_accuracy = sum(batch_accuracies) / len(batch_accuracies) if batch_accuracies else 0.0\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}, Epoch Accuracy: {epoch_accuracy:.2f}%, Average Batch Accuracy: {avg_batch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Switch the model to evaluation mode\n",
    "# vit_model.eval()\n",
    "\n",
    "# # Initialize lists to store all true labels and all predictions\n",
    "# all_labels = []\n",
    "# all_predictions = []\n",
    "\n",
    "# # Disabling gradient calculation\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (images, labels) in enumerate(valid_loader):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = vit_model(images)\n",
    "\n",
    "#         # Get predictions\n",
    "#         _, predicted = outputs.max(1)\n",
    "\n",
    "#         # Store predictions and true labels for later calculation of metrics\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#         # Print status for every 175 batches (or another number if you prefer)\n",
    "#         if (batch_idx + 1) % 175 == 0:\n",
    "#             print(f\"Test Batch [{batch_idx+1}/{len(valid_loader)}] Processed\")\n",
    "\n",
    "# # Compute metrics\n",
    "# accuracy = accuracy_score(all_labels, all_predictions)\n",
    "# precision = precision_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "# recall = recall_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "# f1 = f1_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Test Precision: {precision:.4f}\")\n",
    "# print(f\"Test Recall: {recall:.4f}\")\n",
    "# print(f\"Test F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "save_path = './Vit_mura_phase_3.pth'\n",
    "torch.save(vit_model,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e9a75a43024a5ca03a6659de257b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4180df7932b74b5b98cf8d164329a31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/113M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "save_path = './SWIN_mura_phase_3.pth'\n",
    "torch.save(vit_model,save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
