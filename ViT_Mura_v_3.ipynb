{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './MURA-v1.1/'\n",
    "train_image_paths_csv = \"train_image_paths.csv\"\n",
    "df_train_images_paths = pd.read_csv(os.path.join(path,train_image_paths_csv),dtype=str,header=None)\n",
    "df_train_images_paths.columns = ['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images_paths['label'] = df_train_images_paths['image_path'].map(lambda x: 'positive' if 'positive' in x else 'negative')\n",
    "df_train_images_paths['category'] = df_train_images_paths['image_path'].apply(lambda x: x.split('/')[2])\n",
    "#df_train_images_paths['patientId'] = df_train_images_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_paths_csv = \"valid_image_paths.csv\"\n",
    "df_valid_data_paths = pd.read_csv(os.path.join(path,valid_image_paths_csv),dtype=str,header=None)\n",
    "df_valid_data_paths.columns = ['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_data_paths['label'] = df_valid_data_paths['image_path'].map(lambda x:'positive' if 'positive' in x else 'negative')\n",
    "df_valid_data_paths['category']  = df_valid_data_paths['image_path'].apply(lambda x: x.split('/')[2])\n",
    "#df_valid_data_paths['dir'] =  df_valid_data_paths['image_path'].apply(lambda x: x.split('/')[1])\n",
    "#df_valid_data_paths['patientId']  = df_valid_data_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_data_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByRatio(dataframe,n_samples = int):\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for category in dataframe[\"category\"].unique():\n",
    "        # Filter the DataFrame by category\n",
    "        sub_df = dataframe[dataframe[\"category\"] == category]\n",
    "\n",
    "        # Filter positive and negative samples\n",
    "        pos_samples = sub_df[sub_df[\"label\"] == 'positive'].sample(n=n_samples, replace=False)\n",
    "        neg_samples = sub_df[sub_df[\"label\"] == 'negative'].sample(n=n_samples, replace=False)\n",
    "\n",
    "        # Concatenate positive and negative samples\n",
    "        balanced_sub_df = pd.concat([pos_samples, neg_samples])\n",
    "\n",
    "        # Add to the final balanced DataFrame\n",
    "        balanced_df = pd.concat([balanced_df, balanced_sub_df])\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = splitByRatio(df_train_images_paths,500)\n",
    "balanced_df = balanced_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_balanced = splitByRatio(df_valid_data_paths,100)\n",
    "valid_balanced = valid_balanced.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_train_images_paths.loc[df_train_images_paths['label'] == 'negative']\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images_paths[\"label\"] = df_train_images_paths[\"label\"].replace({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_data_paths[\"label\"] = df_valid_data_paths[\"label\"].replace({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_balanced[\"label\"] = valid_balanced[\"label\"].replace({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feautre extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import prewitt\n",
    "from skimage.filters import gabor\n",
    "from skimage.filters import sobel\n",
    "from skimage.filters import butterworth\n",
    "from skimage.filters import gaussian\n",
    "from skimage.filters import hessian\n",
    "from skimage.filters import median\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, filtered_df.__len__())\n",
    "img = plt.imread(filtered_df['image_path'][idx])\n",
    "plt.imshow(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_img = exposure.equalize_adapthist(img, clip_limit=0.02)\n",
    "plt.imshow(equalized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd, hog_image = hog(equalized_img, orientations=9, pixels_per_cell=(2, 2),\n",
    "                    cells_per_block=(1, 1), visualize=True)\n",
    "plt.imshow(hog_image)\n",
    "fd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prewitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_prewitt = prewitt(equalized_img)\n",
    "plt.imshow(edge_prewitt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_real, gabor_filter = gabor(equalized_img,frequency=0.1)\n",
    "\n",
    "plt.imshow(gabor_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canny_edge = canny(equalized_img,sigma=0)\n",
    "plt.imshow(Canny_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_edge = sobel(equalized_img)\n",
    "plt.imshow(sobel_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butterworth_img = butterworth(equalized_img)\n",
    "plt.imshow(butterworth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_img = gaussian(equalized_img)\n",
    "plt.imshow(gaussian_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_img = hessian(equalized_img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_img = median(equalized_img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.color import rgb2gray\n",
    "class MuraDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, device='cpu'):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.item()\n",
    "        \n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # Convert PIL Image to numpy array\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        image_gray = rgb2gray(image_np)\n",
    "        \n",
    "        # Apply adaptive histogram equalization\n",
    "        image_gray = exposure.equalize_adapthist(image_gray, clip_limit=0.02)\n",
    "\n",
    "        \n",
    "        # Apply Butterworth filter\n",
    "        image_butterworth = butterworth(image_gray)\n",
    "\n",
    "        \n",
    "        # Apply Gaussian filter\n",
    "        image_gaussian = gaussian(image_gray)\n",
    "        \n",
    "        # Combine the two images into one (stack them along the last axis)\n",
    "        # Create a 3 channel image from the butterworth and gaussian filtered images\n",
    "        image_combined = np.stack([image_butterworth, image_gaussian, image_gray], axis=-1)\n",
    "        \n",
    "        image_combined = (image_combined - np.min(image_combined)) / (np.max(image_combined) - np.min(image_combined))\n",
    "        \n",
    "        # Convert to 8-bit image for PIL\n",
    "        image_combined = img_as_ubyte(image_combined)\n",
    "        \n",
    "        # Convert numpy array back to PIL Image\n",
    "        image = Image.fromarray(image_combined)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        image = image.to(self.device)\n",
    "        \n",
    "        label = torch.tensor(label).to(self.device)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_path    3197\n",
       "label         3197\n",
       "category      3197\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_data_paths.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "batchsize = 12\n",
    "transform = transforms.Compose([\n",
    "    v2.RandomHorizontalFlip(p=1),\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor(),\n",
    "])\n",
    "Mura_transform = MuraDataset(df_train_images_paths[\"image_path\"], df_train_images_paths[\"label\"], transform=transform,device='cuda')\n",
    "train_loader = DataLoader(Mura_transform, batch_size=batchsize, shuffle=True)\n",
    "Mura_transform_valid = MuraDataset(df_valid_data_paths[\"image_path\"], df_valid_data_paths[\"label\"], transform=transform,device='cuda')\n",
    "valid_loader = DataLoader(Mura_transform_valid, batch_size=batchsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(valid_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# Select a single image from the batch.\n",
    "img = train_features[6]\n",
    "\n",
    "# Select one channel (e.g., the first channel).\n",
    "img_channel1 = img[0].cpu().numpy()\n",
    "\n",
    "# Display the selected channel.\n",
    "plt.imshow(img_channel1, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Alternatively, you can display the second channel\n",
    "# img_channel2 = img[1].cpu().numpy()\n",
    "# plt.imshow(img_channel2, cmap=\"gray\")\n",
    "# plt.show()\n",
    "\n",
    "label = train_labels[0]\n",
    "print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\timm\\models\\_factory.py:114: UserWarning: Mapping deprecated model name vit_base_patch16_224_in21k to current vit_base_patch16_224.augreg_in21k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "vit_model = timm.create_model(model_name='vit_base_patch16_224_in21k',num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(vit_model.parameters(), lr=1e-7)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 3\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     vit_model.train()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0  # To keep track of correct predictions\n",
    "#     total = 0  # To keep track of total predictions\n",
    "\n",
    "#     batch_accuracies = []  # To store batch accuracies for the epoch\n",
    "\n",
    "#     for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = vit_model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Calculate accuracy\n",
    "#         _, predicted = outputs.max(1)  # Get the index of the max log-probability\n",
    "#         total += labels.size(0)\n",
    "#         correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "#         # Print loss and accuracy for every 584 batches\n",
    "#         if (batch_idx + 1) % 32 == 0:\n",
    "#             batch_accuracy = 100. * correct / total\n",
    "#             batch_accuracies.append(batch_accuracy)  # Store the batch accuracy\n",
    "#             print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Batch Loss: {loss.item():.4f}, Batch Accuracy: {batch_accuracy:.2f}%\")\n",
    "#     scheduler.step()\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     epoch_accuracy = 100. * correct / total  # Convert to percentage\n",
    "\n",
    "#     # Calculate the average of batch accuracies for the epoch\n",
    "#     avg_batch_accuracy = sum(batch_accuracies) / len(batch_accuracies) if batch_accuracies else 0.0\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}, Epoch Accuracy: {epoch_accuracy:.2f}%, Average Batch Accuracy: {avg_batch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Switch the model to evaluation mode\n",
    "# vit_model.eval()\n",
    "\n",
    "# # Initialize lists to store all true labels and all predictions\n",
    "# all_labels = []\n",
    "# all_predictions = []\n",
    "\n",
    "# # Disabling gradient calculation\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (images, labels) in enumerate(valid_loader):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = vit_model(images)\n",
    "\n",
    "#         # Get predictions\n",
    "#         _, predicted = outputs.max(1)\n",
    "\n",
    "#         # Store predictions and true labels for later calculation of metrics\n",
    "#         all_predictions.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#         # Print status for every 175 batches (or another number if you prefer)\n",
    "#         if (batch_idx + 1) % 175 == 0:\n",
    "#             print(f\"Test Batch [{batch_idx+1}/{len(valid_loader)}] Processed\")\n",
    "\n",
    "# # Compute metrics\n",
    "# accuracy = accuracy_score(all_labels, all_predictions)\n",
    "# precision = precision_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "# recall = recall_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "# f1 = f1_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Test Precision: {precision:.4f}\")\n",
    "# print(f\"Test Recall: {recall:.4f}\")\n",
    "# print(f\"Test F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# save_path = './Vit_mura_phase_3.pth'\n",
    "# torch.save(vit_model,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinForImageClassification(\n",
       "  (swin): SwinModel(\n",
       "    (embeddings): SwinEmbeddings(\n",
       "      (patch_embeddings): SwinPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): SwinEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-5): 6 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [100/3068], Batch Loss: 0.6500, Batch Accuracy: 54.25%\n",
      "Epoch [1/5], Batch [200/3068], Batch Loss: 0.6483, Batch Accuracy: 57.50%\n",
      "Epoch [1/5], Batch [300/3068], Batch Loss: 0.7329, Batch Accuracy: 57.69%\n",
      "Epoch [1/5], Batch [400/3068], Batch Loss: 0.5690, Batch Accuracy: 57.00%\n",
      "Epoch [1/5], Batch [500/3068], Batch Loss: 0.7538, Batch Accuracy: 57.62%\n",
      "Epoch [1/5], Batch [600/3068], Batch Loss: 0.5750, Batch Accuracy: 58.19%\n",
      "Epoch [1/5], Batch [700/3068], Batch Loss: 0.6655, Batch Accuracy: 58.86%\n",
      "Epoch [1/5], Batch [800/3068], Batch Loss: 0.7503, Batch Accuracy: 59.21%\n",
      "Epoch [1/5], Batch [900/3068], Batch Loss: 0.4913, Batch Accuracy: 59.73%\n",
      "Epoch [1/5], Batch [1000/3068], Batch Loss: 0.4577, Batch Accuracy: 60.41%\n",
      "Epoch [1/5], Batch [1100/3068], Batch Loss: 0.4840, Batch Accuracy: 60.33%\n",
      "Epoch [1/5], Batch [1200/3068], Batch Loss: 0.8134, Batch Accuracy: 60.45%\n",
      "Epoch [1/5], Batch [1300/3068], Batch Loss: 0.5869, Batch Accuracy: 60.87%\n",
      "Epoch [1/5], Batch [1400/3068], Batch Loss: 0.4024, Batch Accuracy: 61.40%\n",
      "Epoch [1/5], Batch [1500/3068], Batch Loss: 0.4747, Batch Accuracy: 61.83%\n",
      "Epoch [1/5], Batch [1600/3068], Batch Loss: 0.6279, Batch Accuracy: 62.38%\n",
      "Epoch [1/5], Batch [1700/3068], Batch Loss: 0.7306, Batch Accuracy: 62.87%\n",
      "Epoch [1/5], Batch [1800/3068], Batch Loss: 0.6641, Batch Accuracy: 63.32%\n",
      "Epoch [1/5], Batch [1900/3068], Batch Loss: 0.4465, Batch Accuracy: 63.83%\n",
      "Epoch [1/5], Batch [2000/3068], Batch Loss: 0.6328, Batch Accuracy: 64.24%\n",
      "Epoch [1/5], Batch [2100/3068], Batch Loss: 0.5366, Batch Accuracy: 64.57%\n",
      "Epoch [1/5], Batch [2200/3068], Batch Loss: 0.3702, Batch Accuracy: 64.88%\n",
      "Epoch [1/5], Batch [2300/3068], Batch Loss: 0.5516, Batch Accuracy: 65.20%\n",
      "Epoch [1/5], Batch [2400/3068], Batch Loss: 0.5852, Batch Accuracy: 65.55%\n",
      "Epoch [1/5], Batch [2500/3068], Batch Loss: 0.3801, Batch Accuracy: 65.91%\n",
      "Epoch [1/5], Batch [2600/3068], Batch Loss: 0.8110, Batch Accuracy: 66.10%\n",
      "Epoch [1/5], Batch [2700/3068], Batch Loss: 0.4265, Batch Accuracy: 66.37%\n",
      "Epoch [1/5], Batch [2800/3068], Batch Loss: 0.6108, Batch Accuracy: 66.74%\n",
      "Epoch [1/5], Batch [2900/3068], Batch Loss: 0.7295, Batch Accuracy: 67.03%\n",
      "Epoch [1/5], Batch [3000/3068], Batch Loss: 0.6866, Batch Accuracy: 67.22%\n",
      "Epoch [1/5], Average Loss: 0.6032, Epoch Accuracy: 67.39%, Average Batch Accuracy: 62.05%\n",
      "Epoch [2/5], Batch [100/3068], Batch Loss: 0.5001, Batch Accuracy: 75.75%\n",
      "Epoch [2/5], Batch [200/3068], Batch Loss: 0.2015, Batch Accuracy: 76.25%\n",
      "Epoch [2/5], Batch [300/3068], Batch Loss: 0.7262, Batch Accuracy: 76.08%\n",
      "Epoch [2/5], Batch [400/3068], Batch Loss: 0.4617, Batch Accuracy: 76.10%\n",
      "Epoch [2/5], Batch [500/3068], Batch Loss: 0.5374, Batch Accuracy: 76.42%\n",
      "Epoch [2/5], Batch [600/3068], Batch Loss: 0.3523, Batch Accuracy: 76.49%\n",
      "Epoch [2/5], Batch [700/3068], Batch Loss: 0.6987, Batch Accuracy: 76.81%\n",
      "Epoch [2/5], Batch [800/3068], Batch Loss: 0.4980, Batch Accuracy: 76.97%\n",
      "Epoch [2/5], Batch [900/3068], Batch Loss: 0.4580, Batch Accuracy: 76.70%\n",
      "Epoch [2/5], Batch [1000/3068], Batch Loss: 0.4872, Batch Accuracy: 76.42%\n",
      "Epoch [2/5], Batch [1100/3068], Batch Loss: 0.3832, Batch Accuracy: 76.60%\n",
      "Epoch [2/5], Batch [1200/3068], Batch Loss: 0.3815, Batch Accuracy: 76.49%\n",
      "Epoch [2/5], Batch [1300/3068], Batch Loss: 0.2847, Batch Accuracy: 76.56%\n",
      "Epoch [2/5], Batch [1400/3068], Batch Loss: 0.8138, Batch Accuracy: 76.55%\n",
      "Epoch [2/5], Batch [1500/3068], Batch Loss: 0.4707, Batch Accuracy: 76.63%\n",
      "Epoch [2/5], Batch [1600/3068], Batch Loss: 0.5073, Batch Accuracy: 76.67%\n",
      "Epoch [2/5], Batch [1700/3068], Batch Loss: 0.6588, Batch Accuracy: 76.70%\n",
      "Epoch [2/5], Batch [1800/3068], Batch Loss: 0.4899, Batch Accuracy: 76.72%\n",
      "Epoch [2/5], Batch [1900/3068], Batch Loss: 0.7756, Batch Accuracy: 76.78%\n",
      "Epoch [2/5], Batch [2000/3068], Batch Loss: 0.5391, Batch Accuracy: 76.71%\n",
      "Epoch [2/5], Batch [2100/3068], Batch Loss: 0.5517, Batch Accuracy: 76.60%\n",
      "Epoch [2/5], Batch [2200/3068], Batch Loss: 0.4285, Batch Accuracy: 76.69%\n",
      "Epoch [2/5], Batch [2300/3068], Batch Loss: 0.3653, Batch Accuracy: 76.75%\n",
      "Epoch [2/5], Batch [2400/3068], Batch Loss: 0.2584, Batch Accuracy: 76.77%\n",
      "Epoch [2/5], Batch [2500/3068], Batch Loss: 0.6262, Batch Accuracy: 76.81%\n",
      "Epoch [2/5], Batch [2600/3068], Batch Loss: 0.5744, Batch Accuracy: 76.89%\n",
      "Epoch [2/5], Batch [2700/3068], Batch Loss: 0.7211, Batch Accuracy: 76.88%\n",
      "Epoch [2/5], Batch [2800/3068], Batch Loss: 0.7964, Batch Accuracy: 76.90%\n",
      "Epoch [2/5], Batch [2900/3068], Batch Loss: 0.4138, Batch Accuracy: 76.77%\n",
      "Epoch [2/5], Batch [3000/3068], Batch Loss: 0.6007, Batch Accuracy: 76.66%\n",
      "Epoch [2/5], Average Loss: 0.5013, Epoch Accuracy: 76.66%, Average Batch Accuracy: 76.60%\n",
      "Epoch [3/5], Batch [100/3068], Batch Loss: 0.4784, Batch Accuracy: 79.50%\n",
      "Epoch [3/5], Batch [200/3068], Batch Loss: 0.1982, Batch Accuracy: 80.04%\n",
      "Epoch [3/5], Batch [300/3068], Batch Loss: 0.5011, Batch Accuracy: 79.47%\n",
      "Epoch [3/5], Batch [400/3068], Batch Loss: 0.2393, Batch Accuracy: 79.44%\n",
      "Epoch [3/5], Batch [500/3068], Batch Loss: 0.5487, Batch Accuracy: 79.40%\n",
      "Epoch [3/5], Batch [600/3068], Batch Loss: 0.5164, Batch Accuracy: 79.86%\n",
      "Epoch [3/5], Batch [700/3068], Batch Loss: 0.9975, Batch Accuracy: 79.44%\n",
      "Epoch [3/5], Batch [800/3068], Batch Loss: 0.4942, Batch Accuracy: 79.52%\n",
      "Epoch [3/5], Batch [900/3068], Batch Loss: 0.4244, Batch Accuracy: 79.13%\n",
      "Epoch [3/5], Batch [1000/3068], Batch Loss: 0.2007, Batch Accuracy: 79.41%\n",
      "Epoch [3/5], Batch [1100/3068], Batch Loss: 0.4334, Batch Accuracy: 79.43%\n",
      "Epoch [3/5], Batch [1200/3068], Batch Loss: 0.6438, Batch Accuracy: 79.51%\n",
      "Epoch [3/5], Batch [1300/3068], Batch Loss: 0.4428, Batch Accuracy: 79.67%\n",
      "Epoch [3/5], Batch [1400/3068], Batch Loss: 0.5676, Batch Accuracy: 79.64%\n",
      "Epoch [3/5], Batch [1500/3068], Batch Loss: 0.5246, Batch Accuracy: 79.49%\n",
      "Epoch [3/5], Batch [1600/3068], Batch Loss: 0.2791, Batch Accuracy: 79.49%\n",
      "Epoch [3/5], Batch [1700/3068], Batch Loss: 0.4012, Batch Accuracy: 79.48%\n",
      "Epoch [3/5], Batch [1800/3068], Batch Loss: 0.6431, Batch Accuracy: 79.53%\n",
      "Epoch [3/5], Batch [1900/3068], Batch Loss: 0.5379, Batch Accuracy: 79.54%\n",
      "Epoch [3/5], Batch [2000/3068], Batch Loss: 0.3675, Batch Accuracy: 79.55%\n",
      "Epoch [3/5], Batch [2100/3068], Batch Loss: 0.4896, Batch Accuracy: 79.59%\n",
      "Epoch [3/5], Batch [2200/3068], Batch Loss: 0.7629, Batch Accuracy: 79.52%\n",
      "Epoch [3/5], Batch [2300/3068], Batch Loss: 0.5624, Batch Accuracy: 79.39%\n",
      "Epoch [3/5], Batch [2400/3068], Batch Loss: 0.4451, Batch Accuracy: 79.28%\n",
      "Epoch [3/5], Batch [2500/3068], Batch Loss: 0.6289, Batch Accuracy: 79.28%\n",
      "Epoch [3/5], Batch [2600/3068], Batch Loss: 0.6553, Batch Accuracy: 79.24%\n",
      "Epoch [3/5], Batch [2700/3068], Batch Loss: 0.4252, Batch Accuracy: 79.23%\n",
      "Epoch [3/5], Batch [2800/3068], Batch Loss: 0.5928, Batch Accuracy: 79.24%\n",
      "Epoch [3/5], Batch [2900/3068], Batch Loss: 0.6253, Batch Accuracy: 79.20%\n",
      "Epoch [3/5], Batch [3000/3068], Batch Loss: 0.3496, Batch Accuracy: 79.24%\n",
      "Epoch [3/5], Average Loss: 0.4597, Epoch Accuracy: 79.24%, Average Batch Accuracy: 79.46%\n",
      "Epoch [4/5], Batch [100/3068], Batch Loss: 0.3274, Batch Accuracy: 82.75%\n",
      "Epoch [4/5], Batch [200/3068], Batch Loss: 0.2401, Batch Accuracy: 82.62%\n",
      "Epoch [4/5], Batch [300/3068], Batch Loss: 0.3983, Batch Accuracy: 82.64%\n",
      "Epoch [4/5], Batch [400/3068], Batch Loss: 0.5770, Batch Accuracy: 82.27%\n",
      "Epoch [4/5], Batch [500/3068], Batch Loss: 0.6328, Batch Accuracy: 82.10%\n",
      "Epoch [4/5], Batch [600/3068], Batch Loss: 0.3772, Batch Accuracy: 82.14%\n",
      "Epoch [4/5], Batch [700/3068], Batch Loss: 0.7003, Batch Accuracy: 82.27%\n",
      "Epoch [4/5], Batch [800/3068], Batch Loss: 0.3086, Batch Accuracy: 82.41%\n",
      "Epoch [4/5], Batch [900/3068], Batch Loss: 0.2326, Batch Accuracy: 82.31%\n",
      "Epoch [4/5], Batch [1000/3068], Batch Loss: 0.7925, Batch Accuracy: 82.07%\n",
      "Epoch [4/5], Batch [1100/3068], Batch Loss: 0.1381, Batch Accuracy: 82.14%\n",
      "Epoch [4/5], Batch [1200/3068], Batch Loss: 0.8686, Batch Accuracy: 82.19%\n",
      "Epoch [4/5], Batch [1300/3068], Batch Loss: 0.4341, Batch Accuracy: 82.29%\n",
      "Epoch [4/5], Batch [1400/3068], Batch Loss: 0.5778, Batch Accuracy: 82.12%\n",
      "Epoch [4/5], Batch [1500/3068], Batch Loss: 0.3900, Batch Accuracy: 82.17%\n",
      "Epoch [4/5], Batch [1600/3068], Batch Loss: 0.5578, Batch Accuracy: 82.11%\n",
      "Epoch [4/5], Batch [1700/3068], Batch Loss: 0.1921, Batch Accuracy: 82.01%\n",
      "Epoch [4/5], Batch [1800/3068], Batch Loss: 0.3028, Batch Accuracy: 81.88%\n",
      "Epoch [4/5], Batch [1900/3068], Batch Loss: 0.3206, Batch Accuracy: 81.86%\n",
      "Epoch [4/5], Batch [2000/3068], Batch Loss: 0.4092, Batch Accuracy: 81.84%\n",
      "Epoch [4/5], Batch [2100/3068], Batch Loss: 0.3456, Batch Accuracy: 81.81%\n",
      "Epoch [4/5], Batch [2200/3068], Batch Loss: 0.5168, Batch Accuracy: 81.77%\n",
      "Epoch [4/5], Batch [2300/3068], Batch Loss: 0.5368, Batch Accuracy: 81.79%\n",
      "Epoch [4/5], Batch [2400/3068], Batch Loss: 0.1495, Batch Accuracy: 81.82%\n",
      "Epoch [4/5], Batch [2500/3068], Batch Loss: 0.2131, Batch Accuracy: 81.75%\n",
      "Epoch [4/5], Batch [2600/3068], Batch Loss: 0.4700, Batch Accuracy: 81.71%\n",
      "Epoch [4/5], Batch [2700/3068], Batch Loss: 0.4510, Batch Accuracy: 81.72%\n",
      "Epoch [4/5], Batch [2800/3068], Batch Loss: 0.3738, Batch Accuracy: 81.74%\n",
      "Epoch [4/5], Batch [2900/3068], Batch Loss: 0.4810, Batch Accuracy: 81.77%\n",
      "Epoch [4/5], Batch [3000/3068], Batch Loss: 0.5239, Batch Accuracy: 81.77%\n",
      "Epoch [4/5], Average Loss: 0.4203, Epoch Accuracy: 81.83%, Average Batch Accuracy: 82.06%\n",
      "Epoch [5/5], Batch [100/3068], Batch Loss: 0.3621, Batch Accuracy: 83.67%\n",
      "Epoch [5/5], Batch [200/3068], Batch Loss: 0.2861, Batch Accuracy: 83.58%\n",
      "Epoch [5/5], Batch [300/3068], Batch Loss: 0.3437, Batch Accuracy: 83.67%\n",
      "Epoch [5/5], Batch [400/3068], Batch Loss: 0.2488, Batch Accuracy: 83.88%\n",
      "Epoch [5/5], Batch [500/3068], Batch Loss: 0.4019, Batch Accuracy: 83.57%\n",
      "Epoch [5/5], Batch [600/3068], Batch Loss: 0.2567, Batch Accuracy: 83.74%\n",
      "Epoch [5/5], Batch [700/3068], Batch Loss: 0.3165, Batch Accuracy: 84.00%\n",
      "Epoch [5/5], Batch [800/3068], Batch Loss: 0.5466, Batch Accuracy: 84.21%\n",
      "Epoch [5/5], Batch [900/3068], Batch Loss: 0.1434, Batch Accuracy: 84.45%\n",
      "Epoch [5/5], Batch [1000/3068], Batch Loss: 0.9711, Batch Accuracy: 84.40%\n",
      "Epoch [5/5], Batch [1100/3068], Batch Loss: 0.4706, Batch Accuracy: 84.48%\n",
      "Epoch [5/5], Batch [1200/3068], Batch Loss: 0.2765, Batch Accuracy: 84.47%\n",
      "Epoch [5/5], Batch [1300/3068], Batch Loss: 0.1270, Batch Accuracy: 84.53%\n",
      "Epoch [5/5], Batch [1400/3068], Batch Loss: 0.3086, Batch Accuracy: 84.53%\n",
      "Epoch [5/5], Batch [1500/3068], Batch Loss: 0.2162, Batch Accuracy: 84.46%\n",
      "Epoch [5/5], Batch [1600/3068], Batch Loss: 0.1042, Batch Accuracy: 84.47%\n",
      "Epoch [5/5], Batch [1700/3068], Batch Loss: 0.3373, Batch Accuracy: 84.51%\n",
      "Epoch [5/5], Batch [1800/3068], Batch Loss: 0.5461, Batch Accuracy: 84.50%\n",
      "Epoch [5/5], Batch [1900/3068], Batch Loss: 0.2195, Batch Accuracy: 84.44%\n",
      "Epoch [5/5], Batch [2000/3068], Batch Loss: 0.6375, Batch Accuracy: 84.36%\n",
      "Epoch [5/5], Batch [2100/3068], Batch Loss: 0.5993, Batch Accuracy: 84.31%\n",
      "Epoch [5/5], Batch [2200/3068], Batch Loss: 0.3075, Batch Accuracy: 84.17%\n",
      "Epoch [5/5], Batch [2300/3068], Batch Loss: 0.1859, Batch Accuracy: 84.20%\n",
      "Epoch [5/5], Batch [2400/3068], Batch Loss: 0.1993, Batch Accuracy: 84.24%\n",
      "Epoch [5/5], Batch [2500/3068], Batch Loss: 0.7093, Batch Accuracy: 84.20%\n",
      "Epoch [5/5], Batch [2600/3068], Batch Loss: 0.3264, Batch Accuracy: 84.22%\n",
      "Epoch [5/5], Batch [2700/3068], Batch Loss: 0.7878, Batch Accuracy: 84.18%\n",
      "Epoch [5/5], Batch [2800/3068], Batch Loss: 0.2381, Batch Accuracy: 84.18%\n",
      "Epoch [5/5], Batch [2900/3068], Batch Loss: 0.5795, Batch Accuracy: 84.18%\n",
      "Epoch [5/5], Batch [3000/3068], Batch Loss: 0.7309, Batch Accuracy: 84.22%\n",
      "Epoch [5/5], Average Loss: 0.3743, Epoch Accuracy: 84.17%, Average Batch Accuracy: 84.20%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0  # To keep track of correct predictions\n",
    "    total = 0  # To keep track of total predictions\n",
    "\n",
    "    batch_accuracies = []  # To store batch accuracies for the epoch\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits  # Extract the logits\n",
    "        loss = criterion(logits, labels)  # Use the logits her\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = logits.max(1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Print loss and accuracy for every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            batch_accuracy = 100. * correct / total\n",
    "            batch_accuracies.append(batch_accuracy)  # Store the batch accuracy\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Batch Loss: {loss.item():.4f}, Batch Accuracy: {batch_accuracy:.2f}%\")\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_accuracy = 100. * correct / total  # Convert to percentage\n",
    "\n",
    "    # Calculate the average of batch accuracies for the epoch\n",
    "    avg_batch_accuracy = sum(batch_accuracies) / len(batch_accuracies) if batch_accuracies else 0.0\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}, Epoch Accuracy: {epoch_accuracy:.2f}%, Average Batch Accuracy: {avg_batch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch [50/267] Processed\n",
      "Test Batch [100/267] Processed\n",
      "Test Batch [150/267] Processed\n",
      "Test Batch [200/267] Processed\n",
      "Test Batch [250/267] Processed\n",
      "Test Accuracy: 0.7623\n",
      "Test Precision: 0.7748\n",
      "Test Recall: 0.7623\n",
      "Test F1-Score: 0.7578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store all true labels and all predictions\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Disabling gradient calculation\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(valid_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)  # This line was missing in your original code\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = logits.max(1)\n",
    "\n",
    "        # Store predictions and true labels for later calculation of metrics\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Print status for every 50 batches (or another number if you prefer)\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Test Batch [{batch_idx+1}/{len(valid_loader)}] Processed\")\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')  # Using weighted average if you have class imbalance\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epoch\n",
    "Test Accuracy: 0.7623\n",
    "Test Precision: 0.7748\n",
    "Test Recall: 0.7623\n",
    "Test F1-Score: 0.7578\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "save_path = './SWIN_mura_phase_3.pth'\n",
    "torch.save(vit_model,save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
